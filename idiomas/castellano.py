textos = {
    "seleccionar_idioma": "Selecciona el idioma", 
    
    #Opciones preguntas   
    "totalmente_en_desacuerdo" : "Totalmente desacuerdo",
    "en_desacuerdo": "Desacuerdo",
    "neutral": "Neutral",
    "de_acuerdo": "Acuerdo",
    "totalmente_de_acuerdo": "Totalmente acuerdo",
    
    
    "cuestionario": "Cuestionario",    
    
    "boton_continuar": "Siguiente", 
    "boton_atras": "Atrás",
    "boton_enviar": "Enviar",
    "boton_empezar": "Empezar", 
    
    "inicio_cuestionario": "Voler al incio del cuestionario.", 
    
    "error_correo": "Por favor, ingresa un correo electrónico válido.",
    "enviado_con_éxtio": "Enviado con éxito!",
    "error_terminos": "Debes leer y aceptar los términos para continuar.", 
    
    
    "Seccion_1": "Sección 1",
    "Seccion_2": "Sección 2",
    "Seccion_3": "Sección 3",
    
    #Acesso al cuestionario
    "empezar_cuestionario":"¿Quieres empezar el cuestionario?",
    "leer_terminos":"Antes de comenzar, por favor, lee los siguientes términos y condiciones.", 
    "confirmo_leer_terminos": "Confirmo que he leído todo el documento", 
    "acepto_terminos": "He leído y acepto la hoja de información y el consentimiento informado del proyecto, y doy mi consentimiento para participar en el estudio y para el tratamiento de mis datos personales.", 
    
    
    # Preguntas Sección 1: Información Personal 
    "información_personal": "Información Personal",
    "info_personal": "Información Personal", 
    "pregunta_nombre": "Nombre (opcional):",
    "opciones_nombre": "Si quieres formar parte del **sorteo**, por favor ingresa tu **Nombre y Apellido:** ",
    "pregunta_genero": "Género:",
    "genero_opciones": ["Femenino", "Masculino", "No binario", "Prefiero no decirlo"],
    "pregunta_edad": "**Edad:**",
    "edad_opciones": ["Menor de 18", "18 - 24", "25 - 34", "35 - 44", "45 - 54","55 - 64", "Mayor de 64", "Prefiero no decirlo"],
    "pregunta_correo": "**Correo Electrónico (opcional):**", 
    "opcion_correo": "Si quieres formar parte del **sorteo**, por favor ingresa el **correo electrónico:** ",
    "pregunta_nivel_estudios": "**Nivel de estudios**",
    "opciones_nivel_estudios": ["Estudios secundarios", "Estudios universitarios (o superiores a los secundarios)", "Sin estudios", "Otros"], 
    "pregunta_rama_estudios" : "**Por favor, selecciona la rama que mejor describa tus estudios:** ", 
    "opciones_rama_estudios": ["Artes y Humanidades (Diseño, Filosofía, Historia, Traducción e Interpretación, ...)", "Ciencias (Ciencias Ambientales, Física, Geología, Matemáticas, Química, ...)", "Ciencias de la Salud (Enfermería, Medicina, Psicología, Odontología, Veterinaria, ...", "Ciencias Sociales y Jurídicas (ADE, Comunicación Audiovisual, Criminología, Derecho, Economía, Periodismo, Turismo, ...)", "Biociencias (Biología, Bioquímica, Genética, Microbiología, ...)", "Ingenierías y Arquitectura (Ingeniería, Industriales, Informática, Telecos, ...)", "Otros", "Sin estudios"],
    "pregunta_años_experiencia": "**¿Cuántos años de experiencia tienes en este ámbito?: **", 
    "opciones_años_experiencia": ["Menos de 1 año", "1 - 3 años", "4 - 6 años", " 7 - 10 años", "Más de 10 años", "Sin estudios"],  
    "pregunta_pais_residencia": "**¿En qué país resides?**",
    "Gracias_por_contestar_el_formulario": "Muchas gracias por contestar el formulario!",
    
    "otros_opcion": "Por favor, especifique:", ######añadir
    
    # Preguntas Sección 2: Conocimiento sobre la IA
    
    "pregunta_2_1": "Selecciona la opción que describa mejor la inteligencia artificial (IA):",
    "opciones_2_1": ["Campo científico de la informática centrado en la creación de programas considerados inteligentes", "Capacidad de las máquinas para usar algoritmos, aprender de los datos y utilizar lo aprendido en la toma de decisiones tal y como lo haría un ser humano", "Ambas respuestas son correctas" ], 
    
    "pregunta_2_2": "¿Has utilizado alguna vez la inteligencia artificial (IA)?",
    "opciones_2_2": ["Sí, diariamente", "Sí, semanalmente", "Sí, mensualmente", "Sí, esporádicamente", "Nunca" ], 
    
    "pregunta_2_3": "Si has utilizado la IA, ¿para qué la utilizas principalmente? (Puedes seleccionar más de una respuesta)", 
    "opciones_2_3": ["Uso personal", "Estudio académico", "Trabajo", "Otros" ], 
    
    "pregunta_2_4": "¿En qué ámbito utilizas con más frecuencia la IA?",
    "opciones_2_4": ["Arte y diseño", "Ciencias (Matemáticas, Física, Biología,...)", "Ciencias sociales (Economía, Historia,...)", "Humanidades (Lengua, literatura, idiomas,...)", "Negocios y finanzas", "Programación / Ingeniería", "No la utilizo", "Otros"], 
    
    "nota": "Nota: ", 
    "sesgo": "sesgo", 
    "un": "Un ", 
    "intro_preguntas_sesgos": "es un juicio o interpretación que no es objetiva.",
    "pregunta_2_5": "¿Crees que los sistemas de inteligencia artificial pueden tomar decisiones sesgadas (injustas o discriminatorias)?", 
    "opciones_2_5": ["Sí, porque aprenden de datos que pueden estar sesgados por la sociedad", "No, porque la IA analiza los datos de forma neutral", "No estoy seguro/a"], 
        
    "pregunta_2_6": "¿Crees que las personas estamos condicionadas (es decir, que tenemos sesgos) a la hora de tomar decisiones, aunque no nos demos cuenta?", 
    "opciones_2_6": ["Sí, siempre tenemos algún tipo de sesgo", "A menudo, en función del contexto", "No, las personas podemos decidir de forma totalmente objetiva"], 
    
    "pregunta_2_7": "¿Quién cree que posee más sesgos en la toma de decisiones, una persona o un sistema con IA?", 
    "opciones_2_7": ["Una persona", "Un sistema con IA"], 
    
    "pregunta_2_8":"En tu opinión, ¿quién debería ser el principal responsable de evitar los sesgos en los sistemas de inteligencia artificial?", 
    "opciones_2_8":["Las empresas que desarrollan los algoritmos", "Los gobiernos y organismos reguladores", "Los usuarios, que deben usar la IA de forma crítica", "Todos los anteriores comparten responsabilidad"], 
    
    #Preguntas Sección 3: Casos concretos + ética
    "caso_1": "Caso 1",
    "caso_2": "Caso 2",
    "caso_3": "Caso 3",
    "intro_q33" : "En noviembre del 2022, Jake Moffatt compró un vuelo a última hora desde la aerolínea Air Canada para poder asistir al funeral de su abuela. Antes de comprar los billetes, buscó descuentos por fallecimiento de familiares. Hablando con el chatbot de la compañía, le dijo que tenía 90 días desde la compra del billete para completar el formulario de solicitud de reembolso.” Después del evento, solicitó la devolución, pero la empresa se lo denegó, ya que en la página web había un apartado que decía que el descuento se debía solicitar antes del viaje.", 
    "pregunta_3_3": "¿Quién crees que tenía razón en este caso?",
    "opciones_3_3": ["La empresa Air Canada", "Jake Moffatt"], 
    
    "intro_q34":  "Jake llevó a juicio a la compañía. Air Canada argumentó que su página web contenía la información correcta sobre cómo obtener el descuento y que lo que decía el chatbot no era válido. Sin embargo, en el juicio ese argumento no fue suficiente, ya que no había manera de demostrar que la página web era más fiable que la información proporcionada por el chatbot. Por lo que, Air Canada terminó perdiendo el caso y tuvo que pagar a Jake Moffatt.", 
    "pregunta_3_4": "¿Qué opina del veredicto? ",
    "opciones_3_4": ["Estoy de acuerdo", "No estoy de acuerdo" ], 

    "pregunta_3_5": "El profesor Joan Fontrodona de la IESE (Instituto de Estudios Superiores de la Empresa) dijo en una entrevista que todo sistema de inteligencia artificial debía cumplir 3 principios éticos. ¿Cuánto de acuerdo estás con cada uno de ellos?",
    
    "primer_principio": "Primer principio: ",
    "pregunta_3_5_1": " “Respeto de la dignidad humana, lo que significa que todos estos sistemas deben actuar en favor de la dignidad humana”.", 
    "opciones_3_5_1": ["Totalmente desacuerdo", "Desacuerdo","Neutral","Acuerdo","Totalmente acuerdo"], 

    "segundo_principio": "Segundo principio: ",
    "pregunta_3_5_2": "“Libertad. Los sistemas de IA deben respetar y promover la libertad”.", 
    "opciones_3_5_2": ["Totalmente desacuerdo", "Desacuerdo","Neutral","Acuerdo","Totalmente acuerdo"], 

    "tercer_principio": "Tercer principio: ",
    "pregunta_3_5_3": "“Justicia. No todo el mundo tiene acceso a los mismos sistemas, y se dice que los sistemas de IA provocan más división y más desigualdad entre las personas”.", 
    "opciones_3_5_3": ["Totalmente desacuerdo", "Desacuerdo","Neutral","Acuerdo","Totalmente acuerdo"], 

    "pregunta_3_6": "Indica cuánto de acuerdo estás con la siguiente afirmación: “Las tecnologías basadas en la IA ya están siendo utilizadas para ayudar a los humanos a beneficiarse de mejoras significativas y disfrutar de una mayor eficiencia en casi todos los ámbitos de la vida.”", 
    "opciones_3_6": ["Totalmente desacuerdo", "Desacuerdo","Neutral","Acuerdo","Totalmente acuerdo"], 
    
    "pregunta_3_7": "Indica cuánto de acuerdo estás con la siguiente afirmación: La IA también será capaz de ofrecernos sugerencias y predicciones relacionadas con asuntos importantes de nuestra vida, lo que tendrá su impacto en áreas como la salud, el bienestar, la educación, el trabajo y las relaciones interpersonales. De la misma manera, cambiará la forma de hacer negocios al proporcionar ventajas competitivas a las empresas que busquen entender y aplicar estas herramientas de forma rápida y eficaz.",
    "opciones_3_7": ["Totalmente desacuerdo", "Desacuerdo","Neutral","Acuerdo","Totalmente acuerdo"],
    
    "intro_q38_1": "En 2023, un joven de 17 años comenzó a utilizar un chatbot de inteligencia artificial con el que mantenía conversaciones frecuentes. La IA estaba diseñada para simular interacciones humanas de forma muy realista, incluyendo contenido emocional e incluso de tipo romántico o sexualizado. El chico, que ya sufría problemas de salud mental y aislamiento social, desarrolló una relación de fuerte dependencia emocional con el chatbot.", 
    "intro_q38_2": "Durante varias conversaciones, el joven expresó pensamientos autodestructivos y habló abiertamente sobre su intención de suicidarse. A pesar de esto, el chatbot no mostró ningún tipo de respuesta que lo disuadiera ni alertó a ningún sistema de ayuda. De hecho, en lugar de frenar la conversación, siguió interactuando con él como si fuera una persona más, sin filtrar ni limitar el contenido sensible.",  
    "intro_q38_3": "Días después, el joven se quitó la vida.", 
    "intro_q38_4": "Su madre ha denunciado a los desarrolladores del chatbot por negligencia y falta de medidas de protección, alegando que la IA contribuyó significativamente a la muerte de su hijo.", 
    
    "pregunta_3_8_1": "¿Piensas que este tipo de IAs, capaces de mantener conversaciones emocionales realistas, deberían estar disponibles para cualquier usuario?",
    "opciones_3_8_1": ["No, deberían estar restringidas por edad ", "Sí, siempre que el usuario acepte los términos de uso","Deberían estar etiquetadas con advertencias sobre su contenido"],

    "pregunta_3_8_2": "¿Quién crees que tiene mayor responsabilidad en este caso?",
    "opciones_3_8_2": ["El joven estadounidense de 17 años", "La empresa desarrolladora de la IA.","El entorno del joven (familiares, escuela, amigos, etc.)"],

    "intro_caso_3_1": "De manera similar, en 2023, un hombre belga, padre de dos niños pequeños, acabó con su vida tras una conversación de seis semanas sobre la crisis climática con un chatbot de inteligencia artificial llamado Eliza, disponible en una aplicación llamada Chai. Eliza le animó a quitarse la vida después de que él propusiera sacrificarse/suicidarse para salvar el planeta, ya que había perdido la fe en la humanidad para encontrar una solución al calentamiento global y había depositado todas sus esperanzas en la tecnología y la inteligencia artificial para superarlo.",
    "intro_caso_3_2": "Según el medio de noticias, La Libre, que revisó los registros de las conversaciones de texto entre el hombre y el chatbot, “Eliza alimentó sus preocupaciones, lo que agravó su ansiedad y más adelante derivó en pensamientos suicidas”.",

    "pregunta_3_9_1": "En este caso, ¿quién consideras más responsable?",
    "opciones_3_9_1": ["El hombre que mantenía la conversación", "La empresa responsable del chatbot","La plataforma (Chai) que permitió la interacción"],

    "pregunta_3_9_2": "En General, ¿Crees que la IA conversacional debería estar obligada a detectar señales de crisis emocional (como ideación suicida) y actuar de alguna forma (por ejemplo, alertar a una persona real o detener la conversación)?",
    "opciones_3_9_2": ["Sí, debería ser un requisito mínimo", "No, la responsabilidad final siempre debería recaer en el usuario","Depende del tipo de IA y su finalidad"],
    
    #Textos main
    "navegacion": "Navegación", 
    "selecciona_una_seccion": "Selecciona una sección:", 
    "contacto" : "Contacto", 
    "sobre_nosotros" : "Sobre Nosotros",
    "info_sobre_nosotros": "Este cuestionario forma parte del proyecto final de mi grado.", 
    "info_contacto": "Si deseas ponerte en contacto con nosotros, por favor completa el siguiente formulario:", 
    "correo_electronico_contacto": "Correo electrónico: ", 
    "mensaje": "Mensaje: ", 
    "enviar": "Enviar", 
    "mensaje_enviado": "¡Mensaje enviado con éxito!", 
    
    #Errores
    "error_genero": "Por favor, ingresa el género",
    "error_correo": "Correo electrónico no válido",
    "error_age": "Por favor, ingresa la edad",
    "error_nivel_estudios": "Por favor, ingresa el nivel de estudios",
    "error_rama_estudios": "Por favor, ingresa la rama de estudios", 
    "error_años_experiencia" : "Por favor, ingresa cuántos años de experiencia tienes en el ámbito de estudios",
    "error_pais_residencia": "Por favor, ingresa el país de residencia", 
    
    "video_no_encontrado":"El video no se encontró en la ruta especificada",
    "selecciona_opción": "Por favor, selecciona una opción antes de continuar",
}